---
title: "Course Project"
author: "Veronica Vaca"
date: "23 de Marzo de 2018"
output:
  html_document: default
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

The goal of this project is to predict the manner in which some people perform barbell lifts. We will use some variables in the data to do it. We are going to show:

- How the model was built.
- How was used cross validation in it.
- Expected out of sample error. 
- Conclusion and explanation. 
- Predict 20 different test cases.

## Data

The training data for this project are available here:
https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv

The test data are available here:
https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

The data for this project come from this source: http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har.

```{r,echo=FALSE}
#Load libraries
library(caret);library(rpart);library(rpart.plot);library(RColorBrewer)
library(rattle);library(randomForest);library(knitr);require(RANN)

#Download the data
if(!file.exists("pml-training.csv")){download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv", destfile = "pml-training.csv")}

if(!file.exists("pml-testing.csv")){download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv", destfile = "pml-testing.csv")}


#Read the training data and replace empty values by NA
trainingDataSet<- read.csv("pml-training.csv", sep=",", header=TRUE, na.strings = c("NA","",'#DIV/0!'))
testingDataSet<- read.csv("pml-testing.csv", sep=",", header=TRUE, na.strings = c("NA","",'#DIV/0!'))

```

## Cleaning the data

Removing variables that are not usable that have nearly zero variance, variables that are almost always NA, and variables that donâ€™t make intuitive sense for prediction.

```{r, echo =F}

trainingDataSet <- trainingDataSet[,(colSums(is.na(trainingDataSet)) == 0)]

testingDataSet <- testingDataSet[,(colSums(is.na(testingDataSet)) == 0)]


numericalsIdx <- which(lapply(trainingDataSet, class) %in% "numeric")

preprocessModel <-preProcess(trainingDataSet[,numericalsIdx],method=c('knnImpute', 'center', 'scale'))
pre_trainingDataSet <- predict(preprocessModel, trainingDataSet[,numericalsIdx])
pre_trainingDataSet$classe <- trainingDataSet$classe

pre_testingDataSet <-predict(preprocessModel,testingDataSet[,numericalsIdx])


nzv <- nearZeroVar(pre_trainingDataSet,saveMetrics=TRUE)
pre_trainingDataSet <- pre_trainingDataSet[,nzv$nzv==FALSE]

nzv <- nearZeroVar(pre_testingDataSet,saveMetrics=TRUE)
pre_testingDataSet <- pre_testingDataSet[,nzv$nzv==FALSE]

```


## Building the model

For reproducibility we are going to set a seed and then subseting for having a training and a test set.

```{r}
set.seed(2000)

idxTrain<- createDataPartition(pre_trainingDataSet$classe, p=3/4, list=FALSE)
training<- pre_trainingDataSet[idxTrain, ]
validation <- pre_trainingDataSet[-idxTrain, ]
dim(training) ; dim(validation)

```

We will start with the Decision Trees

```{r}
modFitdt<-rpart(classe ~ ., data=training, method="class")
fancyRpartPlot(modFitdt,cex=.3,under.cex=0.2,shadow.offset=0)

```

Now we'll see the accuracy

```{r}
predictiondt <- predict(modFitdt, validation, type = "class")

cmtr<-confusionMatrix(validation$classe,predictiondt)
cmtr
```

We have an accuracy of 0.69


```{r}
plot(cmtr$table, col = cmtr$byClass, main = paste("Decission Trees Confusion Matrix: Accuracy =", round(cmtr$overall['Accuracy'], 4)))

```


## Random Forest

Then we'll test the Random Forest for comparisson

```{r}

set.seed(4444)
modFitrf <- randomForest(classe ~ ., data=training)
predictionrf <- predict(modFitrf, validation, type = "class")
cmrf2 <- confusionMatrix(predictionrf, validation$classe)
cmrf2


```

The random forest model has a 99.4% accuracy, far superior to the rpart method. The specificity and sensitivity is in the high 90s for all variables. 

```{r}
plot(cmrf2$table, col = cmrf2$byClass, main = paste("Random Forest Confusion Matrix: Accuracy =", round(cmrf2$overall['Accuracy'], 4)))

```


## Cross Validation

The cross-validated error for the decision tree is scaled down for easier reading; the error bars on the plot show one standard deviation of the x-validated error.

```{r pressure, echo=FALSE}
plotcp(modFitdt)
```

For random forests, there is no need for cross-validation or a separate test set to get an unbiased estimate of the test set error. It is estimated internally , during the run.


## In Sample & Out of Sample Error


The in sample error is error rate when the model is used to predict the training set it is based off. This error is going to be much less than the model predicting another dataset (out of sample error).

```{r}
insampledt <- predict(modFitdt, training, type = "class")
confusionMatrix(insampledt, training$classe)
insamplerf <- predict(modFitrf, training, type = "class")
confusionMatrix(insamplerf, training$classe)

```

We have for decission trees that the accuracy is over 60% the in sample error is by 40%
when we use random forests the submitted answer resulted in 100% with 0% in sample error. This could be a sign of overfitting. 

The out of sample error will be tested with other samples of subjets. 

## Conclusion
Random Forest was a superior model for prediction of exercise quality compared to rpart decission trees, had over 99% accuracy and fitted well to other subsamples of the data. However, the algorithm may not have as high of accuracy on other samples, particularly ones with different subjects.

